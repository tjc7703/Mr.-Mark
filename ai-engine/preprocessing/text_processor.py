#!/usr/bin/env python3
"""
í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
í† í°í™”, ì •ê·œí™”, ë¶ˆìš©ì–´ ì œê±°, ê°ì • ë¶„ì„ ë“±
"""

import re
import logging
from typing import List, Dict, Any, Optional
import numpy as np
from datetime import datetime
import json
from pathlib import Path
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
import emoji
import unicodedata

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextProcessor:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ê¸°"""

    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words("english"))

        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì¶”ê°€
        korean_stop_words = {
            "ì´",
            "ê·¸",
            "ì €",
            "ê²ƒ",
            "ìˆ˜",
            "ë“±",
            "ë•Œ",
            "ê³³",
            "ë§",
            "ì¼",
            "ë•Œë¬¸",
            "ê·¸ê²ƒ",
            "ê·¸ëŸ°",
            "ì´ëŸ°",
            "ì €ëŸ°",
            "ì–´ë–¤",
            "ë¬´ìŠ¨",
            "ì–´ëŠ",
            "ì•„ë¬´",
            "ëª¨ë“ ",
            "ê°",
            "ì—¬ëŸ¬",
            "ë‹¤ë¥¸",
            "ê°™ì€",
            "ë¹„ìŠ·í•œ",
            "ìƒˆë¡œìš´",
            "ì˜›ë‚ ",
            "í˜„ì¬",
            "ë¯¸ë˜",
            "ê³¼ê±°",
            "ì§€ê¸ˆ",
            "ì´ì œ",
            "ê·¸ë•Œ",
            "ì €ë•Œ",
        }
        self.stop_words.update(korean_stop_words)

        # ê°ì • ë¶„ì„ì„ ìœ„í•œ ê°ì • ì‚¬ì „
        self.sentiment_dict = self._load_sentiment_dict()

    def _load_sentiment_dict(self) -> Dict[str, float]:
        """ê°ì • ì‚¬ì „ ë¡œë“œ"""
        return {
            # ê¸ì •ì  ë‹¨ì–´ë“¤
            "ì¢‹ë‹¤": 1.0,
            "í›Œë¥­í•˜ë‹¤": 1.0,
            "ë©‹ì§€ë‹¤": 1.0,
            "ì™„ë²½í•˜ë‹¤": 1.0,
            "ìµœê³ ": 1.0,
            "ìµœê³ ë‹¤": 1.0,
            "ì‚¬ë‘": 1.0,
            "ì‚¬ë‘í•œë‹¤": 1.0,
            "ê°ë™": 1.0,
            "ê°ë™ì ": 1.0,
            "í–‰ë³µ": 1.0,
            "í–‰ë³µí•˜ë‹¤": 1.0,
            "ì¦ê²ë‹¤": 1.0,
            "ì¬ë¯¸ìˆë‹¤": 1.0,
            "ìœ ìš©í•˜ë‹¤": 1.0,
            "ë„ì›€ì´": 1.0,
            # ë¶€ì •ì  ë‹¨ì–´ë“¤
            "ë‚˜ì˜ë‹¤": -1.0,
            "ìµœì•…": -1.0,
            "ìµœì•…ì´ë‹¤": -1.0,
            "ì‹«ë‹¤": -1.0,
            "ì§œì¦": -1.0,
            "ì§œì¦ë‚˜ë‹¤": -1.0,
            "í™”ë‚˜ë‹¤": -1.0,
            "ë¶„ë…¸": -1.0,
            "ì‹¤ë§": -1.0,
            "ì‹¤ë§í•˜ë‹¤": -1.0,
            "ìŠ¬í”„ë‹¤": -1.0,
            "ìš°ìš¸í•˜ë‹¤": -1.0,
            "í˜ë“¤ë‹¤": -1.0,
            "ì–´ë µë‹¤": -1.0,
            "ë³µì¡í•˜ë‹¤": -1.0,
            "ë¶ˆí¸í•˜ë‹¤": -1.0,
            # ì¤‘ë¦½ì  ë‹¨ì–´ë“¤
            "ë³´í†µ": 0.0,
            "ì¼ë°˜ì ": 0.0,
            "í‰ë²”í•˜ë‹¤": 0.0,
            "ê·¸ì €": 0.0,
            "ê·¸ëƒ¥": 0.0,
            "ê·¸ëŒ€ë¡œ": 0.0,
            "ë™ì¼í•˜ë‹¤": 0.0,
            "ê°™ë‹¤": 0.0,
        }

    def preprocess_text(self, text: str, options: Dict[str, bool] = None) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        if not text:
            return ""

        if options is None:
            options = {
                "normalize": True,
                "remove_emoji": True,
                "remove_urls": True,
                "remove_hashtags": False,
                "remove_mentions": True,
                "remove_numbers": False,
                "lowercase": True,
                "remove_stopwords": True,
                "lemmatize": True,
            }

        try:
            # ê¸°ë³¸ ì •ê·œí™”
            if options.get("normalize", True):
                text = self._normalize_text(text)

            # ì´ëª¨ì§€ ì œê±°
            if options.get("remove_emoji", True):
                text = self._remove_emoji(text)

            # URL ì œê±°
            if options.get("remove_urls", True):
                text = self._remove_urls(text)

            # í•´ì‹œíƒœê·¸ ì œê±° (ì„ íƒì )
            if options.get("remove_hashtags", False):
                text = self._remove_hashtags(text)

            # ë©˜ì…˜ ì œê±°
            if options.get("remove_mentions", True):
                text = self._remove_mentions(text)

            # ìˆ«ì ì œê±° (ì„ íƒì )
            if options.get("remove_numbers", False):
                text = self._remove_numbers(text)

            # ì†Œë¬¸ì ë³€í™˜
            if options.get("lowercase", True):
                text = text.lower()

            # í† í°í™”
            tokens = self._tokenize(text)

            # ë¶ˆìš©ì–´ ì œê±°
            if options.get("remove_stopwords", True):
                tokens = self._remove_stopwords(tokens)

            # í‘œì œì–´ ì¶”ì¶œ
            if options.get("lemmatize", True):
                tokens = self._lemmatize_tokens(tokens)

            # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì¬ì¡°í•©
            processed_text = " ".join(tokens)

            return processed_text

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return text

    def _normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize("NFKC", text)

        # ê³µë°± ì •ê·œí™”
        text = re.sub(r"\s+", " ", text)

        # íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
        text = re.sub(r"[^\w\sê°€-í£]", " ", text)

        return text.strip()

    def _remove_emoji(self, text: str) -> str:
        """ì´ëª¨ì§€ ì œê±°"""
        return emoji.replace_emojis(text, replace="")

    def _remove_urls(self, text: str) -> str:
        """URL ì œê±°"""
        url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        return re.sub(url_pattern, "", text)

    def _remove_hashtags(self, text: str) -> str:
        """í•´ì‹œíƒœê·¸ ì œê±°"""
        hashtag_pattern = r"#\w+"
        return re.sub(hashtag_pattern, "", text)

    def _remove_mentions(self, text: str) -> str:
        """ë©˜ì…˜ ì œê±°"""
        mention_pattern = r"@\w+"
        return re.sub(mention_pattern, "", text)

    def _remove_numbers(self, text: str) -> str:
        """ìˆ«ì ì œê±°"""
        return re.sub(r"\d+", "", text)

    def _tokenize(self, text: str) -> List[str]:
        """í† í°í™”"""
        try:
            return word_tokenize(text)
        except:
            # NLTK í† í°í™” ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜ í† í°í™”
            return text.split()

    def _remove_stopwords(self, tokens: List[str]) -> List[str]:
        """ë¶ˆìš©ì–´ ì œê±°"""
        return [token for token in tokens if token.lower() not in self.stop_words]

    def _lemmatize_tokens(self, tokens: List[str]) -> List[str]:
        """í‘œì œì–´ ì¶”ì¶œ"""
        lemmatized = []
        for token in tokens:
            try:
                # ì˜ì–´ ë‹¨ì–´ëŠ” NLTK lemmatizer ì‚¬ìš©
                if re.match(r"^[a-zA-Z]+$", token):
                    lemmatized.append(self.lemmatizer.lemmatize(token))
                else:
                    # í•œêµ­ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                    lemmatized.append(token)
            except:
                lemmatized.append(token)
        return lemmatized

    def extract_hashtags(self, text: str) -> List[str]:
        """í•´ì‹œíƒœê·¸ ì¶”ì¶œ"""
        hashtag_pattern = r"#(\w+)"
        hashtags = re.findall(hashtag_pattern, text)
        return hashtags

    def extract_mentions(self, text: str) -> List[str]:
        """ë©˜ì…˜ ì¶”ì¶œ"""
        mention_pattern = r"@(\w+)"
        mentions = re.findall(mention_pattern, text)
        return mentions

    def extract_urls(self, text: str) -> List[str]:
        """URL ì¶”ì¶œ"""
        url_pattern = r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
        urls = re.findall(url_pattern, text)
        return urls

    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """ê°ì • ë¶„ì„"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°ì • ë¶„ì„ìš©)
            processed_text = self.preprocess_text(
                text,
                {
                    "normalize": True,
                    "remove_emoji": True,
                    "remove_urls": True,
                    "remove_hashtags": False,
                    "remove_mentions": True,
                    "remove_numbers": False,
                    "lowercase": True,
                    "remove_stopwords": False,  # ê°ì • ë¶„ì„ì—ì„œëŠ” ë¶ˆìš©ì–´ ìœ ì§€
                    "lemmatize": True,
                },
            )

            tokens = processed_text.split()

            # ê°ì • ì ìˆ˜ ê³„ì‚°
            positive_score = 0
            negative_score = 0
            neutral_score = 0

            for token in tokens:
                sentiment_value = self.sentiment_dict.get(token, 0)
                if sentiment_value > 0:
                    positive_score += sentiment_value
                elif sentiment_value < 0:
                    negative_score += abs(sentiment_value)
                else:
                    neutral_score += 1

            # ì •ê·œí™”
            total_tokens = len(tokens)
            if total_tokens > 0:
                positive_score /= total_tokens
                negative_score /= total_tokens
                neutral_score /= total_tokens

            # ì „ì²´ ê°ì • ì ìˆ˜
            overall_sentiment = positive_score - negative_score

            # ê°ì • ë¶„ë¥˜
            if overall_sentiment > 0.1:
                sentiment_label = "positive"
            elif overall_sentiment < -0.1:
                sentiment_label = "negative"
            else:
                sentiment_label = "neutral"

            return {
                "overall_sentiment": overall_sentiment,
                "sentiment_label": sentiment_label,
                "positive_score": positive_score,
                "negative_score": negative_score,
                "neutral_score": neutral_score,
                "confidence": min(abs(overall_sentiment), 1.0),
            }

        except Exception as e:
            logger.error(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return {
                "overall_sentiment": 0.0,
                "sentiment_label": "neutral",
                "positive_score": 0.0,
                "negative_score": 0.0,
                "neutral_score": 1.0,
                "confidence": 0.0,
            }

    def extract_keywords(self, text: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            processed_text = self.preprocess_text(
                text,
                {
                    "normalize": True,
                    "remove_emoji": True,
                    "remove_urls": True,
                    "remove_hashtags": False,
                    "remove_mentions": True,
                    "remove_numbers": False,
                    "lowercase": True,
                    "remove_stopwords": True,
                    "lemmatize": True,
                },
            )

            tokens = processed_text.split()

            # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
            word_freq = {}
            for token in tokens:
                if len(token) > 1:  # 1ê¸€ì ë‹¨ì–´ ì œì™¸
                    word_freq[token] = word_freq.get(token, 0) + 1

            # ë¹ˆë„ìˆœ ì •ë ¬
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

            # ìƒìœ„ kê°œ í‚¤ì›Œë“œ ë°˜í™˜
            keywords = []
            for word, freq in sorted_words[:top_k]:
                keywords.append(
                    {
                        "word": word,
                        "frequency": freq,
                        "weight": freq / len(tokens) if tokens else 0,
                    }
                )

            return keywords

        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def extract_topics(
        self, texts: List[str], num_topics: int = 5
    ) -> List[Dict[str, Any]]:
        """í† í”½ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            all_keywords = {}

            # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            for text in texts:
                keywords = self.extract_keywords(text, top_k=20)
                for keyword in keywords:
                    word = keyword["word"]
                    if word in all_keywords:
                        all_keywords[word]["frequency"] += keyword["frequency"]
                        all_keywords[word]["documents"] += 1
                    else:
                        all_keywords[word]["frequency"] = keyword["frequency"]
                        all_keywords[word]["documents"] = 1

            # TF-IDF ìŠ¤íƒ€ì¼ ì ìˆ˜ ê³„ì‚°
            total_docs = len(texts)
            for word, data in all_keywords.items():
                tf = data["frequency"]
                idf = np.log(total_docs / data["documents"])
                data["tfidf_score"] = tf * idf

            # TF-IDF ì ìˆ˜ìˆœ ì •ë ¬
            sorted_topics = sorted(
                all_keywords.items(), key=lambda x: x[1]["tfidf_score"], reverse=True
            )

            # ìƒìœ„ í† í”½ ë°˜í™˜
            topics = []
            for word, data in sorted_topics[:num_topics]:
                topics.append(
                    {
                        "topic": word,
                        "frequency": data["frequency"],
                        "tfidf_score": data["tfidf_score"],
                        "document_count": data["documents"],
                    }
                )

            return topics

        except Exception as e:
            logger.error(f"í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []

    def batch_process(
        self, texts: List[str], options: Dict[str, bool] = None
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        results = []

        for i, text in enumerate(texts):
            try:
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                processed_text = self.preprocess_text(text, options)

                # ê°ì • ë¶„ì„
                sentiment = self.analyze_sentiment(text)

                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keywords = self.extract_keywords(text)

                # í•´ì‹œíƒœê·¸, ë©˜ì…˜, URL ì¶”ì¶œ
                hashtags = self.extract_hashtags(text)
                mentions = self.extract_mentions(text)
                urls = self.extract_urls(text)

                results.append(
                    {
                        "index": i,
                        "original_text": text,
                        "processed_text": processed_text,
                        "sentiment": sentiment,
                        "keywords": keywords,
                        "hashtags": hashtags,
                        "mentions": mentions,
                        "urls": urls,
                        "word_count": len(processed_text.split()),
                        "char_count": len(text),
                    }
                )

            except Exception as e:
                logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ (ì¸ë±ìŠ¤ {i}): {str(e)}")
                results.append({"index": i, "original_text": text, "error": str(e)})

        return results


def main():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    processor = TextProcessor()

    # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸
    test_texts = [
        "ì˜¤ëŠ˜ ì •ë§ ì¢‹ì€ í•˜ë£¨ì˜€ì–´ìš”! #í–‰ë³µ #ì¢‹ì€ë‚  @ì¹œêµ¬ì™€ í•¨ê»˜",
        "ì´ ì œí’ˆì€ ì •ë§ ìµœê³ ì˜ˆìš”! ì™„ë²½í•œ í’ˆì§ˆì…ë‹ˆë‹¤.",
        "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ë„ˆë¬´ ë‚˜ë¹ ì„œ ì§œì¦ë‚˜ìš”... ğŸ˜¡",
        "ì´ ì˜í™”ëŠ” ê·¸ì € ê·¸ë¬ì–´ìš”. íŠ¹ë³„í•œ ì ì´ ì—†ì—ˆìŠµë‹ˆë‹¤.",
    ]

    # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    results = processor.batch_process(test_texts)

    for result in results:
        print(f"ì›ë³¸: {result['original_text']}")
        print(f"ì „ì²˜ë¦¬: {result['processed_text']}")
        print(
            f"ê°ì •: {result['sentiment']['sentiment_label']} ({result['sentiment']['overall_sentiment']:.2f})"
        )
        print(f"í‚¤ì›Œë“œ: {[kw['word'] for kw in result['keywords'][:5]]}")
        print(f"í•´ì‹œíƒœê·¸: {result['hashtags']}")
        print("---")


if __name__ == "__main__":
    main()
